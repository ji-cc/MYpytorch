{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors类似与NumPy的,唯一的区别是Tensor可以在GPU上加速运行\n",
    "可以定义一些深度学习模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.7245e-39, 9.2755e-39, 8.9082e-39],\n",
       "        [9.9184e-39, 8.4490e-39, 9.6429e-39],\n",
       "        [1.0653e-38, 1.0469e-38, 4.2246e-39],\n",
       "        [1.0378e-38, 9.6429e-39, 9.2755e-39],\n",
       "        [9.7346e-39, 1.0745e-38, 1.0102e-38]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建5*3矩阵，只是分配空间，未初始化\n",
    "x = torch.Tensor(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个未初始化的5*3矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0653e-38, 1.0194e-38, 8.4490e-39],\n",
       "        [1.0469e-38, 9.3674e-39, 9.9184e-39],\n",
       "        [8.7245e-39, 9.2755e-39, 8.9082e-39],\n",
       "        [9.9184e-39, 8.4490e-39, 9.6429e-39],\n",
       "        [1.0653e-38, 1.0469e-38, 4.2246e-39]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())  #查看x的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()[0]    #查看行的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(1)   #查看列的个数，两种写法等价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个随机初始化的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0013, 0.9803, 0.3387],\n",
       "        [0.6857, 0.5501, 0.0112],\n",
       "        [0.0553, 0.9426, 0.7204],\n",
       "        [0.3887, 0.6482, 0.3172],\n",
       "        [0.8924, 0.6909, 0.8008]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个全为0，类型为long的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype  #打印类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)   #类型转换\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3).long()   #类型转换\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据直接构建tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以从已有的tensor构建另一个tensor。\n",
    "这些方法会重用原来的tensor的特征，例如：数据类型，除非提供新的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5, 3)\n",
    "print(x,x.dtype) #跟原来的x一样是浮点型\n",
    "#若要改变数据类型\n",
    "y = x.new_ones(5, 3, dtype = torch.double)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机产生的数字   like表示形状得一样 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5696, -0.1003, -0.5218],\n",
       "        [ 0.0816,  1.2711, -0.8568],\n",
       "        [-0.7871, -0.4898,  0.7647],\n",
       "        [-1.5623, -0.0914,  0.5044],\n",
       "        [ 1.2804, -0.8605,  0.3387]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)    #随机产生跟x形状相同的随机数\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到tensor的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape    #x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加法运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6923, 0.9679, 0.8331],\n",
       "        [0.1266, 0.0457, 0.8368],\n",
       "        [0.3858, 0.8025, 0.6611],\n",
       "        [0.4196, 0.2747, 0.7842],\n",
       "        [0.5247, 0.3939, 0.6175]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0653e-38, 1.0194e-38, 8.4490e-39],\n",
       "        [1.0469e-38, 9.3674e-39, 9.9184e-39],\n",
       "        [8.7245e-39, 9.2755e-39, 8.9082e-39],\n",
       "        [9.9184e-39, 8.4490e-39, 9.6429e-39],\n",
       "        [1.0653e-38, 1.0469e-38, 4.2246e-39]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法的第一种写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6923, 0.9679, 0.8331],\n",
       "        [0.1266, 0.0457, 0.8368],\n",
       "        [0.3858, 0.8025, 0.6611],\n",
       "        [0.4196, 0.2747, 0.7842],\n",
       "        [0.5247, 0.3939, 0.6175]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法的第二种写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6923, 0.9679, 0.8331],\n",
       "        [0.1266, 0.0457, 0.8368],\n",
       "        [0.3858, 0.8025, 0.6611],\n",
       "        [0.4196, 0.2747, 0.7842],\n",
       "        [0.5247, 0.3939, 0.6175]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法的第三种写法:指定加法结果的输出目标为result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6923, 0.9679, 0.8331],\n",
       "        [0.1266, 0.0457, 0.8368],\n",
       "        [0.3858, 0.8025, 0.6611],\n",
       "        [0.4196, 0.2747, 0.7842],\n",
       "        [0.5247, 0.3939, 0.6175]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.Tensor(5,3)    # 预先分配空间\n",
    "torch.add(x,y, out = result)    # 输入到result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法：把输出作为一个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0993,  0.6963, -0.1581],\n",
       "        [ 0.5298,  1.5560, -0.6586],\n",
       "        [-0.4301, -0.2539,  0.8213],\n",
       "        [-1.2690,  0.5434,  1.4106],\n",
       "        [ 1.6871, -0.4403,  1.3381]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "#torch.add(x, y, out=result)\n",
    "result = x + y\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in-place加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0993,  0.6963, -0.1581],\n",
       "        [ 0.5298,  1.5560, -0.6586],\n",
       "        [-0.4301, -0.2539,  0.8213],\n",
       "        [-1.2690,  0.5434,  1.4106],\n",
       "        [ 1.6871, -0.4403,  1.3381]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)   #加_表示把x操作在y中\n",
    "y   #y的值改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark style=background-color:red>\n",
    "注：</mark>\n",
    "任何in-place的运算都会以_结尾。eg:x.copy_(y)，x.t_()，都会改变x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种类似的NumPy的indexing都可以在PyTorch tensor上面使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5696, -0.1003, -0.5218],\n",
       "        [ 0.0816,  1.2711, -0.8568],\n",
       "        [-0.7871, -0.4898,  0.7647],\n",
       "        [-1.5623, -0.0914,  0.5044],\n",
       "        [ 1.2804, -0.8605,  0.3387]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1003, -0.5218],\n",
       "        [ 1.2711, -0.8568],\n",
       "        [-0.4898,  0.7647],\n",
       "        [-0.0914,  0.5044],\n",
       "        [-0.8605,  0.3387]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 1: ]   #把所有的行留下   从第一列开始往后面取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2711, -0.8568],\n",
       "        [-0.4898,  0.7647],\n",
       "        [-0.0914,  0.5044],\n",
       "        [-0.8605,  0.3387]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:, 1: ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing:如果你希望resize/reshape一个tensor，可以使用torch.view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2689,  1.0911,  0.0690, -0.1765],\n",
      "        [ 1.5710,  2.4072,  0.4937, -0.5687],\n",
      "        [-3.5520,  1.3367,  0.9791, -0.9546],\n",
      "        [-0.7522, -1.3900,  0.9307,  0.3008]])\n",
      "tensor([-1.2689,  1.0911,  0.0690, -0.1765,  1.5710,  2.4072,  0.4937, -0.5687,\n",
      "        -3.5520,  1.3367,  0.9791, -0.9546, -0.7522, -1.3900,  0.9307,  0.3008])\n",
      "tensor([[-1.2689,  1.0911,  0.0690, -0.1765,  1.5710,  2.4072,  0.4937, -0.5687],\n",
      "        [-3.5520,  1.3367,  0.9791, -0.9546, -0.7522, -1.3900,  0.9307,  0.3008]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2689,  1.0911,  0.0690, -0.1765,  1.5710,  2.4072,  0.4937, -0.5687],\n",
       "        [-3.5520,  1.3367,  0.9791, -0.9546, -0.7522, -1.3900,  0.9307,  0.3008]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "print(x)\n",
    "y = x.view(16)\n",
    "print(y)\n",
    "z = x.view(2, 8)   #把x变为2x8\n",
    "print(z)\n",
    "b = x.view(2,-1)    #系统可以自动算出来此处的-1是为8    b = x.view(-1,8) 同理：系统可以自动更改-1为2\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果只有一个元素的tensor,使用.item()方法可以把里面的value变成Python数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4930])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ilshift__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rmul__',\n",
       " '__rpow__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_coalesced_',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_indices',\n",
       " '_is_view',\n",
       " '_make_subclass',\n",
       " '_nnz',\n",
       " '_update_names',\n",
       " '_values',\n",
       " '_version',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'align_as',\n",
       " 'align_to',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitwise_not',\n",
       " 'bitwise_not_',\n",
       " 'bitwise_xor',\n",
       " 'bitwise_xor_',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'conj',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'cuda',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'dense_dim',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fft',\n",
       " 'fill_',\n",
       " 'fill_diagonal_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'float',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'gather',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'has_names',\n",
       " 'histc',\n",
       " 'ifft',\n",
       " 'imag',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'int',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'irfft',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_contiguous',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_leaf',\n",
       " 'is_mkldnn',\n",
       " 'is_nonzero',\n",
       " 'is_pinned',\n",
       " 'is_quantized',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'isclose',\n",
       " 'item',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logdet',\n",
       " 'logical_not',\n",
       " 'logical_not_',\n",
       " 'logical_xor',\n",
       " 'logical_xor_',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstsq',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'names',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'ndim',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nonzero',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'put_',\n",
       " 'q_per_channel_axis',\n",
       " 'q_per_channel_scales',\n",
       " 'q_per_channel_zero_points',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qr',\n",
       " 'qscheme',\n",
       " 'random_',\n",
       " 'real',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'refine_names',\n",
       " 'register_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'rename',\n",
       " 'rename_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'repeat_interleave',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'retain_grad',\n",
       " 'rfft',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'select',\n",
       " 'set_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'sum',\n",
       " 'sum_to_size',\n",
       " 'svd',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_mkldnn',\n",
       " 'to_sparse',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unflatten',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'values',\n",
       " 'var',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'where',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.49303799867630005"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy和Tensor之间的转化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch Tensor和Numpy array会共享内存，所以改变其中一项也会改变另一项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把Torch Tensor转变成Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()     # Tensor -> Numpy\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变numpy array里面的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1] = 2\n",
    "b   %改变b的同时a也会改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1., 1., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把numpy array转变成torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   # 后续使用numpy时可以用np代替numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)   # Numpy -> Tensor\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(a, 1, out=a)      #在原始内存空间上改变值\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b   # a与b共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a + 1    #a = a + 1   数值并没有存到原来的a里面，相当于重新定义了一个内存空间\n",
    "print(a)    \n",
    "b    #所以此时的a与b不相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有的CPU上的Tensor都支持转成numpy或者中numpr转成Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用.to方法，Tensor可以被移动到别的device上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()   #因为此时在笔记本上  没有GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():      # 若在电脑上有GPU,则返回true\n",
    "    device = torch.device(\"cuda\")   #cuda是一个GPU运算库\n",
    "    y = torch.ones_like(x, device=device)  # 把Tensor放到GPU上\n",
    "    x = x.to(device)    #与上步操作一样，可以把tensor搬到GPU上\n",
    "    #然后可以进行运算，运算效果会有一个很大的提升\n",
    "    z = x + y\n",
    "    print(z)   #此时z在GPU上\n",
    "    print(z.to(\"cpu\", torch.double))         #把z搬回到CPU上   类型转换为torch.double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #如果y是一个GPU的tensor,是不能直接变成numpy 要先变到cpu上   因为numpy的所有操作是在cpu上\n",
    "y.cpu().data.numpy()   \n",
    "y.to(\"cpu\").data.numpy()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#在GPU上进行操作可以大大提高运算效率，尤其面对大数据的时候\n",
    "#要训练一个很大的模型的时候，CPU上就训练不起来，需要搬到GPU上\n",
    "#把整个模型变成cuda 可以写；model = model.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 热身：用Numpy实现两层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个全连接ReLU神经网络，一个隐藏层，没有bias，用来从x预测y，使用L2 Loss\n",
    "\n",
    "- $h = W_1X$ \n",
    "\n",
    "- $a = max(0, h)$\n",
    "\n",
    "- $y_{hat} = W_2a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用numpy来计算前向神经网络，loss，和反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy ndarray是一个普通的n微array。他不知道任何关于深度学习或者梯度（gradient）的知识，也不知道计算图\n",
    "（computation graph）,只是一种用来计算数学运算的数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32508816.86736097\n",
      "1 26696958.238504488\n",
      "2 21857093.11924529\n",
      "3 16556837.119172925\n",
      "4 11541332.714072587\n",
      "5 7552407.28961717\n",
      "6 4854781.2900699945\n",
      "7 3173703.5329361157\n",
      "8 2167509.452189046\n",
      "9 1559002.058216245\n",
      "10 1177910.3028039997\n",
      "11 926792.8701247531\n",
      "12 752069.2986668046\n",
      "13 624162.7887723373\n",
      "14 526512.0861935796\n",
      "15 449497.2417425005\n",
      "16 387222.5179967876\n",
      "17 335885.18218275555\n",
      "18 293113.8847723546\n",
      "19 257032.4623036594\n",
      "20 226328.811718776\n",
      "21 200044.67902338947\n",
      "22 177425.804693802\n",
      "23 157850.90247764206\n",
      "24 140826.61906261812\n",
      "25 125970.71589296494\n",
      "26 112962.04426345383\n",
      "27 101522.45369528969\n",
      "28 91434.15226335326\n",
      "29 82510.41466953642\n",
      "30 74593.21697868255\n",
      "31 67551.83605983132\n",
      "32 61275.9301353778\n",
      "33 55679.47479572585\n",
      "34 50672.21407206453\n",
      "35 46182.389094529346\n",
      "36 42145.67200724143\n",
      "37 38510.47424723723\n",
      "38 35232.06002107379\n",
      "39 32270.519848330205\n",
      "40 29592.76290642543\n",
      "41 27166.13663923723\n",
      "42 24963.268618096175\n",
      "43 22961.36632218388\n",
      "44 21138.484411171503\n",
      "45 19478.282710409003\n",
      "46 17963.877843632563\n",
      "47 16580.31013503314\n",
      "48 15314.96813219448\n",
      "49 14156.715173501721\n",
      "50 13095.388571142264\n",
      "51 12122.41343065535\n",
      "52 11231.02050116608\n",
      "53 10412.509257151942\n",
      "54 9661.659992892666\n",
      "55 8970.201278260838\n",
      "56 8333.326032976851\n",
      "57 7746.188606516605\n",
      "58 7205.7409038317\n",
      "59 6706.647450909082\n",
      "60 6245.684860765436\n",
      "61 5819.314181120198\n",
      "62 5424.628874577911\n",
      "63 5059.066568432387\n",
      "64 4720.441815510355\n",
      "65 4406.394659635042\n",
      "66 4115.0035118540245\n",
      "67 3844.627169301937\n",
      "68 3593.4588251761784\n",
      "69 3360.0570813192744\n",
      "70 3142.9845488484802\n",
      "71 2941.1446324892877\n",
      "72 2753.3392674737534\n",
      "73 2578.5448569697032\n",
      "74 2415.67047474907\n",
      "75 2264.0273136887536\n",
      "76 2123.193267782062\n",
      "77 1991.7251339228578\n",
      "78 1868.9938337651597\n",
      "79 1754.4483615087267\n",
      "80 1647.4135052910617\n",
      "81 1547.4510068398795\n",
      "82 1453.9741573220556\n",
      "83 1366.5297111971556\n",
      "84 1284.7678190972042\n",
      "85 1208.2452279995973\n",
      "86 1136.628810667625\n",
      "87 1069.4918741597585\n",
      "88 1006.5857297164534\n",
      "89 947.6364253310642\n",
      "90 892.3963661659918\n",
      "91 840.6150599674636\n",
      "92 792.0211249820131\n",
      "93 746.3844188244584\n",
      "94 703.5321812026134\n",
      "95 663.3091794081472\n",
      "96 625.5332992861639\n",
      "97 590.0357078464779\n",
      "98 556.6697423278438\n",
      "99 525.3269406489597\n",
      "100 495.82821919541624\n",
      "101 468.00570726526513\n",
      "102 441.84200329493274\n",
      "103 417.2244374607721\n",
      "104 394.05260087525846\n",
      "105 372.26234525946586\n",
      "106 351.7321232546317\n",
      "107 332.40782366270815\n",
      "108 314.2025669964556\n",
      "109 297.0577330463037\n",
      "110 280.8999870543618\n",
      "111 265.6596133554031\n",
      "112 251.28810035764474\n",
      "113 237.74073848695696\n",
      "114 224.97742143188665\n",
      "115 212.92528812249816\n",
      "116 201.54361657465054\n",
      "117 190.79852401062442\n",
      "118 180.65466960711908\n",
      "119 171.08004513463663\n",
      "120 162.03517032679088\n",
      "121 153.49024174742908\n",
      "122 145.42682302409588\n",
      "123 137.80034226749203\n",
      "124 130.58963487057719\n",
      "125 123.78378212506058\n",
      "126 117.34926491340158\n",
      "127 111.26395272716503\n",
      "128 105.50865019199992\n",
      "129 100.0643523996817\n",
      "130 94.91807448583037\n",
      "131 90.041372202335\n",
      "132 85.42731453883044\n",
      "133 81.05681087520851\n",
      "134 76.91871124292274\n",
      "135 73.00191347781094\n",
      "136 69.29310379450516\n",
      "137 65.78217469529181\n",
      "138 62.45368122076353\n",
      "139 59.29921120155353\n",
      "140 56.3113788858901\n",
      "141 53.47740470602112\n",
      "142 50.792001190197624\n",
      "143 48.242915477270415\n",
      "144 45.82930425288593\n",
      "145 43.54080852466293\n",
      "146 41.36903927482809\n",
      "147 39.3080670960524\n",
      "148 37.35375010567904\n",
      "149 35.49994857581872\n",
      "150 33.74010253656168\n",
      "151 32.07073009840193\n",
      "152 30.488151597516573\n",
      "153 28.985490517566912\n",
      "154 27.55892271601687\n",
      "155 26.203615261094292\n",
      "156 24.917520977660182\n",
      "157 23.69618493426282\n",
      "158 22.53642885725899\n",
      "159 21.436057495098503\n",
      "160 20.39080605357644\n",
      "161 19.397305093298492\n",
      "162 18.45378428169374\n",
      "163 17.557631635161307\n",
      "164 16.706020651357083\n",
      "165 15.89616297354153\n",
      "166 15.126906684656918\n",
      "167 14.396553112317314\n",
      "168 13.701796777937274\n",
      "169 13.041211212494165\n",
      "170 12.41357856106492\n",
      "171 11.81686396081424\n",
      "172 11.24895078241123\n",
      "173 10.709163408608024\n",
      "174 10.196539762253717\n",
      "175 9.708931411858625\n",
      "176 9.24471556570877\n",
      "177 8.803197067566611\n",
      "178 8.38357114446695\n",
      "179 7.983984667056951\n",
      "180 7.60391676120452\n",
      "181 7.2424259524058225\n",
      "182 6.898870217019139\n",
      "183 6.571644121835405\n",
      "184 6.260071932981272\n",
      "185 5.96385108275104\n",
      "186 5.681771663834844\n",
      "187 5.413252398816339\n",
      "188 5.157661901817314\n",
      "189 4.91455980900397\n",
      "190 4.683090444778692\n",
      "191 4.462563406822881\n",
      "192 4.252758434131991\n",
      "193 4.052823043053863\n",
      "194 3.862510532979778\n",
      "195 3.6813418397116284\n",
      "196 3.5087854559617715\n",
      "197 3.344608353443545\n",
      "198 3.188081740254982\n",
      "199 3.0390333105695513\n",
      "200 2.8970202173004584\n",
      "201 2.7618766442123843\n",
      "202 2.6332015429354803\n",
      "203 2.510494731265348\n",
      "204 2.3936940762915535\n",
      "205 2.282262433790775\n",
      "206 2.1761639584086097\n",
      "207 2.075008119186019\n",
      "208 1.9786335553384915\n",
      "209 1.8868262818921666\n",
      "210 1.7994124030521297\n",
      "211 1.7161077383991878\n",
      "212 1.6366910488717012\n",
      "213 1.5609494092175669\n",
      "214 1.4887840607566567\n",
      "215 1.4200033990007266\n",
      "216 1.3544554994915312\n",
      "217 1.2919407133547396\n",
      "218 1.2324501550951872\n",
      "219 1.1757190828349093\n",
      "220 1.1215890408552265\n",
      "221 1.0699483199670072\n",
      "222 1.0207372993088704\n",
      "223 0.9738198131088962\n",
      "224 0.9290706095454748\n",
      "225 0.8864301623861677\n",
      "226 0.8458073008491608\n",
      "227 0.807010545240159\n",
      "228 0.7700233233341236\n",
      "229 0.7347500537971792\n",
      "230 0.701105746431359\n",
      "231 0.6690038370599267\n",
      "232 0.6384311424278337\n",
      "233 0.6092559155536098\n",
      "234 0.581430936014862\n",
      "235 0.5548764335321503\n",
      "236 0.529558148482538\n",
      "237 0.5053926563773332\n",
      "238 0.48237300578313613\n",
      "239 0.46041156988206566\n",
      "240 0.43945700801772264\n",
      "241 0.4194408989229931\n",
      "242 0.40036808386265016\n",
      "243 0.38215059944799434\n",
      "244 0.36477092948727635\n",
      "245 0.3481952877899849\n",
      "246 0.3323800186248394\n",
      "247 0.3172976697756023\n",
      "248 0.3028967225222389\n",
      "249 0.2891596974508052\n",
      "250 0.2760537980015704\n",
      "251 0.2635416134614335\n",
      "252 0.25160260669211915\n",
      "253 0.24020871199388727\n",
      "254 0.229342077220811\n",
      "255 0.21896730121924712\n",
      "256 0.209066336141732\n",
      "257 0.19962117265312174\n",
      "258 0.1906034679233726\n",
      "259 0.1819905101132941\n",
      "260 0.17376947330081755\n",
      "261 0.16593536889231822\n",
      "262 0.15844654148889534\n",
      "263 0.15129770038016263\n",
      "264 0.14447510790218376\n",
      "265 0.13796594882573718\n",
      "266 0.13174637044382503\n",
      "267 0.12581318958102164\n",
      "268 0.12015232758422092\n",
      "269 0.11474329038391436\n",
      "270 0.10957935824119142\n",
      "271 0.10465250148947598\n",
      "272 0.09994679649243082\n",
      "273 0.09545507591509712\n",
      "274 0.09116848320621038\n",
      "275 0.08707966910812509\n",
      "276 0.08316766487745877\n",
      "277 0.07943363166513222\n",
      "278 0.0758680900975015\n",
      "279 0.07246394380452134\n",
      "280 0.06921395922947896\n",
      "281 0.06611158780569845\n",
      "282 0.06315108573331925\n",
      "283 0.060320368304325245\n",
      "284 0.05761799268077024\n",
      "285 0.055037252838892596\n",
      "286 0.05257326394792068\n",
      "287 0.050220692852495\n",
      "288 0.04797414674048417\n",
      "289 0.04582914110085845\n",
      "290 0.04377979826418969\n",
      "291 0.04182126742055987\n",
      "292 0.03995284390565325\n",
      "293 0.03817028492409262\n",
      "294 0.03646470337737314\n",
      "295 0.03483605663238714\n",
      "296 0.033282585596637354\n",
      "297 0.03179616981109583\n",
      "298 0.030376795733219226\n",
      "299 0.02902139180883472\n",
      "300 0.02772692349564509\n",
      "301 0.026489978345214462\n",
      "302 0.025309963890394457\n",
      "303 0.024183103364193163\n",
      "304 0.023105208468144605\n",
      "305 0.022075595432827992\n",
      "306 0.02109215830209538\n",
      "307 0.020152761673288444\n",
      "308 0.019255150288945568\n",
      "309 0.018398577556235557\n",
      "310 0.017580188331750117\n",
      "311 0.016799229992064547\n",
      "312 0.016052400566845933\n",
      "313 0.015338278507582142\n",
      "314 0.014656013293947358\n",
      "315 0.014004632109826755\n",
      "316 0.01338250884923596\n",
      "317 0.012787958328439647\n",
      "318 0.012220061985876097\n",
      "319 0.01167724620997227\n",
      "320 0.011158528278123775\n",
      "321 0.010663287040571293\n",
      "322 0.01019002560120648\n",
      "323 0.009737993217003625\n",
      "324 0.009305763229083672\n",
      "325 0.008893101106060956\n",
      "326 0.008498653355533885\n",
      "327 0.008121612248750625\n",
      "328 0.007761514889663895\n",
      "329 0.007418001896721276\n",
      "330 0.007089488729671103\n",
      "331 0.006775325223087618\n",
      "332 0.006475152636180039\n",
      "333 0.006188309265398355\n",
      "334 0.005914324049100917\n",
      "335 0.005652270164010231\n",
      "336 0.0054021560773578215\n",
      "337 0.005163085577323167\n",
      "338 0.004934570053278926\n",
      "339 0.004716183920358639\n",
      "340 0.0045076350559453025\n",
      "341 0.004308223997226752\n",
      "342 0.004117654317800265\n",
      "343 0.0039356347777489255\n",
      "344 0.00376159360518581\n",
      "345 0.0035952635264978813\n",
      "346 0.0034364145932548875\n",
      "347 0.0032847846472476585\n",
      "348 0.0031396397642448875\n",
      "349 0.003001038911070664\n",
      "350 0.0028684854760795556\n",
      "351 0.0027417442657861344\n",
      "352 0.0026206139624023635\n",
      "353 0.0025049441760259337\n",
      "354 0.0023943559896261743\n",
      "355 0.0022886503227209243\n",
      "356 0.0021877568289996128\n",
      "357 0.002091238574460333\n",
      "358 0.0019989235197256667\n",
      "359 0.001910729936339865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 0.0018264260294313314\n",
      "361 0.0017458863025315366\n",
      "362 0.0016689403238171327\n",
      "363 0.0015953722306289203\n",
      "364 0.0015250915586276603\n",
      "365 0.001457947223046451\n",
      "366 0.0013936823037084144\n",
      "367 0.001332230265464688\n",
      "368 0.0012735353632518792\n",
      "369 0.001217484701464103\n",
      "370 0.001163856518029702\n",
      "371 0.0011125711203577463\n",
      "372 0.0010635772557229823\n",
      "373 0.0010167548533815429\n",
      "374 0.0009719768651424984\n",
      "375 0.0009292027583159484\n",
      "376 0.0008883262479355354\n",
      "377 0.0008492247729534874\n",
      "378 0.0008118445219392386\n",
      "379 0.0007761135546344185\n",
      "380 0.0007419557679709099\n",
      "381 0.0007093382209330145\n",
      "382 0.000678191369097881\n",
      "383 0.0006483835099120267\n",
      "384 0.0006198611616513122\n",
      "385 0.0005926068961858845\n",
      "386 0.0005665446747824658\n",
      "387 0.0005416352772521304\n",
      "388 0.0005178285930669675\n",
      "389 0.0004950926988630306\n",
      "390 0.0004733388421731916\n",
      "391 0.00045253643000126324\n",
      "392 0.0004326464757891611\n",
      "393 0.0004136371401701377\n",
      "394 0.00039546637979945883\n",
      "395 0.00037809648901805003\n",
      "396 0.00036149648601320624\n",
      "397 0.0003456327933365429\n",
      "398 0.0003304535168457602\n",
      "399 0.0003159459558588186\n",
      "400 0.0003020919093817292\n",
      "401 0.00028883048965894876\n",
      "402 0.0002761579547352101\n",
      "403 0.0002640397304504601\n",
      "404 0.0002524578250104187\n",
      "405 0.0002413774851877641\n",
      "406 0.00023078639473524137\n",
      "407 0.0002206604570252872\n",
      "408 0.00021098347371188058\n",
      "409 0.00020172917655872185\n",
      "410 0.00019288427373745723\n",
      "411 0.0001844261865963896\n",
      "412 0.0001763376464834089\n",
      "413 0.00016860608028256694\n",
      "414 0.0001612147029003151\n",
      "415 0.00015414843630115731\n",
      "416 0.00014739084628392267\n",
      "417 0.00014093867854615374\n",
      "418 0.0001347644123320273\n",
      "419 0.00012886292461364678\n",
      "420 0.0001232142982338064\n",
      "421 0.0001178173141912885\n",
      "422 0.00011265507548820126\n",
      "423 0.00010771861885433862\n",
      "424 0.0001030021478619873\n",
      "425 9.848989695049172e-05\n",
      "426 9.417684302908832e-05\n",
      "427 9.005416703495751e-05\n",
      "428 8.611103105725558e-05\n",
      "429 8.233961142604066e-05\n",
      "430 7.873323559362746e-05\n",
      "431 7.528756366462816e-05\n",
      "432 7.199182768650342e-05\n",
      "433 6.8840999826532e-05\n",
      "434 6.58312767674088e-05\n",
      "435 6.295439722198398e-05\n",
      "436 6.019845472489403e-05\n",
      "437 5.7563693826769676e-05\n",
      "438 5.5045496536106914e-05\n",
      "439 5.263631675072671e-05\n",
      "440 5.033563891730454e-05\n",
      "441 4.813454193407887e-05\n",
      "442 4.6028256769513475e-05\n",
      "443 4.401408560012296e-05\n",
      "444 4.208971732191385e-05\n",
      "445 4.024919455154953e-05\n",
      "446 3.848924755742735e-05\n",
      "447 3.68066549402995e-05\n",
      "448 3.519755180755978e-05\n",
      "449 3.365838574214623e-05\n",
      "450 3.218680304211992e-05\n",
      "451 3.0780376195918546e-05\n",
      "452 2.9437525324718875e-05\n",
      "453 2.8151230084830015e-05\n",
      "454 2.6921223434285053e-05\n",
      "455 2.5744449092849296e-05\n",
      "456 2.4619055724976468e-05\n",
      "457 2.3543616401358088e-05\n",
      "458 2.2515269785962064e-05\n",
      "459 2.15319888460009e-05\n",
      "460 2.059114507262649e-05\n",
      "461 1.969177217401808e-05\n",
      "462 1.8831573496694873e-05\n",
      "463 1.8008909669060044e-05\n",
      "464 1.7222346544535342e-05\n",
      "465 1.6470501991678826e-05\n",
      "466 1.575134220132147e-05\n",
      "467 1.506373677336732e-05\n",
      "468 1.4406012747380915e-05\n",
      "469 1.3778017944029815e-05\n",
      "470 1.3176417958429228e-05\n",
      "471 1.2601688279732555e-05\n",
      "472 1.2051592063696823e-05\n",
      "473 1.1525524153648703e-05\n",
      "474 1.1022339914016205e-05\n",
      "475 1.0541312322519404e-05\n",
      "476 1.0081408397489243e-05\n",
      "477 9.641444019914777e-06\n",
      "478 9.22094614578816e-06\n",
      "479 8.818511736530115e-06\n",
      "480 8.433762530409257e-06\n",
      "481 8.065773328209666e-06\n",
      "482 7.713948419040603e-06\n",
      "483 7.377349716727922e-06\n",
      "484 7.0558854990229365e-06\n",
      "485 6.748147575301997e-06\n",
      "486 6.454325359397398e-06\n",
      "487 6.172733582183412e-06\n",
      "488 5.903577926115178e-06\n",
      "489 5.646164789534157e-06\n",
      "490 5.400068228961476e-06\n",
      "491 5.164665775534702e-06\n",
      "492 4.9394337901965596e-06\n",
      "493 4.7240436965194075e-06\n",
      "494 4.518133660759967e-06\n",
      "495 4.321190741864364e-06\n",
      "496 4.132834597285575e-06\n",
      "497 3.952776879509943e-06\n",
      "498 3.7805111730488906e-06\n",
      "499 3.6157247533383434e-06\n"
     ]
    }
   ],
   "source": [
    "# N:输入   D_in：输入1000维   H：输出10维    D_out：中间层100维\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 \n",
    "#随机创建一些训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "learning_rate = 1e-6   \n",
    "for it in range(500):\n",
    "    # forward pass\n",
    "    h = x.dot(w1)   # N * H\n",
    "    h_relu = np.maximum(h, 0)   # N * H\n",
    "    y_pred = h_relu.dot(w2)  # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    # y = ax + b\n",
    "    # dy / dx = a\n",
    "    # dy / da = x\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)    # T 转置\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # compute weight of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch:Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tensors来创建前向神经网络，计算损失，以及反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个Pytorch Tensor很像一个numpy的ndarry，但是他和numpy ndarry最大的区别是：Pytorch Tensor可以在CPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29951818.0\n",
      "1 23308764.0\n",
      "2 19889714.0\n",
      "3 16807886.0\n",
      "4 13469244.0\n",
      "5 10057719.0\n",
      "6 7101619.0\n",
      "7 4825993.0\n",
      "8 3252558.5\n",
      "9 2219624.0\n",
      "10 1560765.75\n",
      "11 1139141.625\n",
      "12 864525.6875\n",
      "13 679481.625\n",
      "14 550044.75\n",
      "15 455717.28125\n",
      "16 384443.0\n",
      "17 328665.25\n",
      "18 283723.84375\n",
      "19 246797.6875\n",
      "20 216064.234375\n",
      "21 190067.75\n",
      "22 167883.9375\n",
      "23 148788.390625\n",
      "24 132268.734375\n",
      "25 117903.5625\n",
      "26 105357.1171875\n",
      "27 94352.078125\n",
      "28 84672.6171875\n",
      "29 76142.2734375\n",
      "30 68603.4765625\n",
      "31 61917.7578125\n",
      "32 55972.8671875\n",
      "33 50676.7578125\n",
      "34 45950.48828125\n",
      "35 41727.25390625\n",
      "36 37942.17578125\n",
      "37 34545.68359375\n",
      "38 31491.33984375\n",
      "39 28744.22265625\n",
      "40 26266.853515625\n",
      "41 24027.1640625\n",
      "42 22002.03515625\n",
      "43 20168.5078125\n",
      "44 18506.25\n",
      "45 16996.375\n",
      "46 15622.6494140625\n",
      "47 14372.2197265625\n",
      "48 13232.912109375\n",
      "49 12193.91796875\n",
      "50 11244.728515625\n",
      "51 10377.12890625\n",
      "52 9583.197265625\n",
      "53 8856.0625\n",
      "54 8189.61865234375\n",
      "55 7578.0751953125\n",
      "56 7016.3603515625\n",
      "57 6499.94873046875\n",
      "58 6024.93798828125\n",
      "59 5587.9765625\n",
      "60 5185.39111328125\n",
      "61 4814.2578125\n",
      "62 4471.84765625\n",
      "63 4155.83837890625\n",
      "64 3864.167724609375\n",
      "65 3594.54296875\n",
      "66 3345.267578125\n",
      "67 3114.586181640625\n",
      "68 2901.05224609375\n",
      "69 2703.36572265625\n",
      "70 2520.134765625\n",
      "71 2350.322509765625\n",
      "72 2192.702880859375\n",
      "73 2046.373291015625\n",
      "74 1910.5264892578125\n",
      "75 1784.3837890625\n",
      "76 1667.0950927734375\n",
      "77 1558.08642578125\n",
      "78 1456.637451171875\n",
      "79 1362.2252197265625\n",
      "80 1274.398681640625\n",
      "81 1192.5587158203125\n",
      "82 1116.3363037109375\n",
      "83 1045.2940673828125\n",
      "84 979.0181274414062\n",
      "85 917.24072265625\n",
      "86 859.572509765625\n",
      "87 805.747314453125\n",
      "88 755.490478515625\n",
      "89 708.5733642578125\n",
      "90 664.7279052734375\n",
      "91 623.7490234375\n",
      "92 585.4332885742188\n",
      "93 549.5931396484375\n",
      "94 516.0845947265625\n",
      "95 484.74932861328125\n",
      "96 455.4256591796875\n",
      "97 427.9530944824219\n",
      "98 402.22802734375\n",
      "99 378.1209716796875\n",
      "100 355.5337829589844\n",
      "101 334.3948974609375\n",
      "102 314.5522155761719\n",
      "103 295.9400634765625\n",
      "104 278.4836120605469\n",
      "105 262.10113525390625\n",
      "106 246.73316955566406\n",
      "107 232.3079833984375\n",
      "108 218.77809143066406\n",
      "109 206.06460571289062\n",
      "110 194.12400817871094\n",
      "111 182.9053497314453\n",
      "112 172.3678741455078\n",
      "113 162.4586181640625\n",
      "114 153.142578125\n",
      "115 144.38958740234375\n",
      "116 136.15293884277344\n",
      "117 128.40597534179688\n",
      "118 121.11714935302734\n",
      "119 114.25640869140625\n",
      "120 107.79923248291016\n",
      "121 101.72220611572266\n",
      "122 96.00418853759766\n",
      "123 90.61652374267578\n",
      "124 85.54193115234375\n",
      "125 80.76158905029297\n",
      "126 76.25839233398438\n",
      "127 72.01713562011719\n",
      "128 68.01944732666016\n",
      "129 64.2872314453125\n",
      "130 60.76626968383789\n",
      "131 57.44641876220703\n",
      "132 54.31626892089844\n",
      "133 51.36507797241211\n",
      "134 48.580421447753906\n",
      "135 45.95329666137695\n",
      "136 43.47396469116211\n",
      "137 41.13372802734375\n",
      "138 38.92567443847656\n",
      "139 36.83879089355469\n",
      "140 34.86845016479492\n",
      "141 33.008094787597656\n",
      "142 31.250904083251953\n",
      "143 29.5907039642334\n",
      "144 28.02264976501465\n",
      "145 26.539695739746094\n",
      "146 25.139291763305664\n",
      "147 23.814599990844727\n",
      "148 22.562875747680664\n",
      "149 21.37843894958496\n",
      "150 20.25858497619629\n",
      "151 19.199569702148438\n",
      "152 18.19820785522461\n",
      "153 17.250598907470703\n",
      "154 16.35409164428711\n",
      "155 15.505250930786133\n",
      "156 14.70223331451416\n",
      "157 13.94186782836914\n",
      "158 13.222482681274414\n",
      "159 12.540861129760742\n",
      "160 11.896085739135742\n",
      "161 11.28520679473877\n",
      "162 10.707100868225098\n",
      "163 10.15876293182373\n",
      "164 9.639735221862793\n",
      "165 9.148253440856934\n",
      "166 8.682085037231445\n",
      "167 8.240389823913574\n",
      "168 7.821972846984863\n",
      "169 7.425326824188232\n",
      "170 7.049635887145996\n",
      "171 6.693200588226318\n",
      "172 6.355537414550781\n",
      "173 6.035028457641602\n",
      "174 5.7314958572387695\n",
      "175 5.44345760345459\n",
      "176 5.170112133026123\n",
      "177 4.911278247833252\n",
      "178 4.665348529815674\n",
      "179 4.4320526123046875\n",
      "180 4.210935592651367\n",
      "181 4.000938892364502\n",
      "182 3.801692008972168\n",
      "183 3.6127431392669678\n",
      "184 3.4332971572875977\n",
      "185 3.263044595718384\n",
      "186 3.101405143737793\n",
      "187 2.9480764865875244\n",
      "188 2.802353620529175\n",
      "189 2.664085626602173\n",
      "190 2.5326554775238037\n",
      "191 2.408074140548706\n",
      "192 2.2894999980926514\n",
      "193 2.176988124847412\n",
      "194 2.0701935291290283\n",
      "195 1.9687683582305908\n",
      "196 1.8724377155303955\n",
      "197 1.7808539867401123\n",
      "198 1.6938509941101074\n",
      "199 1.6110597848892212\n",
      "200 1.5325225591659546\n",
      "201 1.4579253196716309\n",
      "202 1.3870081901550293\n",
      "203 1.3195229768753052\n",
      "204 1.255523443222046\n",
      "205 1.19460928440094\n",
      "206 1.136723279953003\n",
      "207 1.0817300081253052\n",
      "208 1.0293623208999634\n",
      "209 0.9796013832092285\n",
      "210 0.9322985410690308\n",
      "211 0.8872504830360413\n",
      "212 0.8445815443992615\n",
      "213 0.8039634823799133\n",
      "214 0.7652788162231445\n",
      "215 0.7284888029098511\n",
      "216 0.6934779286384583\n",
      "217 0.66024249792099\n",
      "218 0.6286145448684692\n",
      "219 0.5984452962875366\n",
      "220 0.5698837041854858\n",
      "221 0.5426363348960876\n",
      "222 0.5166616439819336\n",
      "223 0.4920477569103241\n",
      "224 0.46854645013809204\n",
      "225 0.44620200991630554\n",
      "226 0.4249570369720459\n",
      "227 0.40472179651260376\n",
      "228 0.3854721188545227\n",
      "229 0.36718109250068665\n",
      "230 0.3497145473957062\n",
      "231 0.3330910801887512\n",
      "232 0.317353755235672\n",
      "233 0.30228278040885925\n",
      "234 0.2879642844200134\n",
      "235 0.2743329703807831\n",
      "236 0.2613774538040161\n",
      "237 0.24902743101119995\n",
      "238 0.23728540539741516\n",
      "239 0.22603586316108704\n",
      "240 0.21539244055747986\n",
      "241 0.20526222884655\n",
      "242 0.19555698335170746\n",
      "243 0.18637172877788544\n",
      "244 0.1776079684495926\n",
      "245 0.16924679279327393\n",
      "246 0.1613045185804367\n",
      "247 0.15377408266067505\n",
      "248 0.14648866653442383\n",
      "249 0.13965082168579102\n",
      "250 0.13309936225414276\n",
      "251 0.12688510119915009\n",
      "252 0.1209259033203125\n",
      "253 0.11525937914848328\n",
      "254 0.10987970978021622\n",
      "255 0.1047658696770668\n",
      "256 0.09984853118658066\n",
      "257 0.09518098831176758\n",
      "258 0.09073608368635178\n",
      "259 0.08652376383543015\n",
      "260 0.0824967473745346\n",
      "261 0.0786537453532219\n",
      "262 0.07498622685670853\n",
      "263 0.07150174677371979\n",
      "264 0.06818856298923492\n",
      "265 0.06501935422420502\n",
      "266 0.06200755760073662\n",
      "267 0.05911821499466896\n",
      "268 0.05638672411441803\n",
      "269 0.05379081517457962\n",
      "270 0.05128386989235878\n",
      "271 0.04891166463494301\n",
      "272 0.04663688316941261\n",
      "273 0.0444871187210083\n",
      "274 0.042428478598594666\n",
      "275 0.040489282459020615\n",
      "276 0.038618750870227814\n",
      "277 0.03682989627122879\n",
      "278 0.035127636045217514\n",
      "279 0.03352037072181702\n",
      "280 0.03197697177529335\n",
      "281 0.030507773160934448\n",
      "282 0.029099535197019577\n",
      "283 0.027762185782194138\n",
      "284 0.026495974510908127\n",
      "285 0.025283750146627426\n",
      "286 0.02412746660411358\n",
      "287 0.02302219532430172\n",
      "288 0.021972257643938065\n",
      "289 0.02095814235508442\n",
      "290 0.020004652440547943\n",
      "291 0.019087746739387512\n",
      "292 0.018219109624624252\n",
      "293 0.017389368265867233\n",
      "294 0.01660277135670185\n",
      "295 0.015851426869630814\n",
      "296 0.015127329155802727\n",
      "297 0.014433410950005054\n",
      "298 0.01378689706325531\n",
      "299 0.013155761174857616\n",
      "300 0.012566709890961647\n",
      "301 0.0120024923235178\n",
      "302 0.011458584107458591\n",
      "303 0.010956021025776863\n",
      "304 0.010459691286087036\n",
      "305 0.00998854823410511\n",
      "306 0.009541910141706467\n",
      "307 0.009112793020904064\n",
      "308 0.008709296584129333\n",
      "309 0.008323455229401588\n",
      "310 0.007954479195177555\n",
      "311 0.007603684440255165\n",
      "312 0.007268862798810005\n",
      "313 0.006951629649847746\n",
      "314 0.006647713948041201\n",
      "315 0.0063544888980686665\n",
      "316 0.006080646067857742\n",
      "317 0.005814901553094387\n",
      "318 0.005565323866903782\n",
      "319 0.005328707862645388\n",
      "320 0.005099876318126917\n",
      "321 0.004878404084593058\n",
      "322 0.004670480731874704\n",
      "323 0.00447149109095335\n",
      "324 0.004277769010514021\n",
      "325 0.004099720157682896\n",
      "326 0.0039252531714737415\n",
      "327 0.003759602317586541\n",
      "328 0.003602844662964344\n",
      "329 0.003454527584835887\n",
      "330 0.003308557905256748\n",
      "331 0.0031706534791737795\n",
      "332 0.0030420259572565556\n",
      "333 0.002916469005867839\n",
      "334 0.002798369387164712\n",
      "335 0.002683028345927596\n",
      "336 0.002577744424343109\n",
      "337 0.0024731699377298355\n",
      "338 0.0023757799062877893\n",
      "339 0.002281543565914035\n",
      "340 0.002188867423683405\n",
      "341 0.002102440455928445\n",
      "342 0.0020225883927196264\n",
      "343 0.0019411196699365973\n",
      "344 0.0018659151392057538\n",
      "345 0.001792401890270412\n",
      "346 0.0017231213860213757\n",
      "347 0.0016580857336521149\n",
      "348 0.0015958152944222093\n",
      "349 0.0015362158883363008\n",
      "350 0.001478040823712945\n",
      "351 0.0014229129301384091\n",
      "352 0.00137031520716846\n",
      "353 0.001319962670095265\n",
      "354 0.0012735738418996334\n",
      "355 0.001226258696988225\n",
      "356 0.0011821910738945007\n",
      "357 0.0011400419753044844\n",
      "358 0.0011000330559909344\n",
      "359 0.0010616765357553959\n",
      "360 0.0010240700794383883\n",
      "361 0.000988638843409717\n",
      "362 0.0009542526677250862\n",
      "363 0.0009226092370226979\n",
      "364 0.0008904518908821046\n",
      "365 0.000859542575199157\n",
      "366 0.0008316760649904609\n",
      "367 0.000803240982349962\n",
      "368 0.0007775039412081242\n",
      "369 0.000751775223761797\n",
      "370 0.0007270137430168688\n",
      "371 0.0007027860847301781\n",
      "372 0.0006819394766353071\n",
      "373 0.0006593254511244595\n",
      "374 0.0006379074766300619\n",
      "375 0.0006172740249894559\n",
      "376 0.0005981356371194124\n",
      "377 0.0005803246167488396\n",
      "378 0.0005612969980575144\n",
      "379 0.0005442575784400105\n",
      "380 0.0005282103084027767\n",
      "381 0.0005121816648170352\n",
      "382 0.0004964358522556722\n",
      "383 0.0004824200295843184\n",
      "384 0.00046788319014012814\n",
      "385 0.00045503280125558376\n",
      "386 0.0004416557203512639\n",
      "387 0.0004285425820853561\n",
      "388 0.00041588611202314496\n",
      "389 0.0004040049680043012\n",
      "390 0.00039197993464767933\n",
      "391 0.0003812263021245599\n",
      "392 0.00037115730810910463\n",
      "393 0.00036089718923904\n",
      "394 0.00035135640064254403\n",
      "395 0.0003420831635594368\n",
      "396 0.00033312878804281354\n",
      "397 0.0003247088461648673\n",
      "398 0.00031583610689267516\n",
      "399 0.00030779745429754257\n",
      "400 0.0002996983239427209\n",
      "401 0.0002922618587035686\n",
      "402 0.00028436415595933795\n",
      "403 0.0002770058927126229\n",
      "404 0.00027016783133149147\n",
      "405 0.0002630308154039085\n",
      "406 0.000256990606430918\n",
      "407 0.00024998464505188167\n",
      "408 0.0002441625401843339\n",
      "409 0.00023862374655436724\n",
      "410 0.0002330010902369395\n",
      "411 0.00022771875956095755\n",
      "412 0.00022197769430931658\n",
      "413 0.00021654076408594847\n",
      "414 0.0002113250084221363\n",
      "415 0.0002059528633253649\n",
      "416 0.0002012402837863192\n",
      "417 0.0001969380973605439\n",
      "418 0.00019299973791930825\n",
      "419 0.00018834856746252626\n",
      "420 0.00018394894141238183\n",
      "421 0.0001795924035832286\n",
      "422 0.00017629431386012584\n",
      "423 0.00017215925618074834\n",
      "424 0.00016789486107882112\n",
      "425 0.00016407089424319565\n",
      "426 0.00016063514340203255\n",
      "427 0.00015705518308095634\n",
      "428 0.00015426594472955912\n",
      "429 0.00015105360944289714\n",
      "430 0.00014787859981879592\n",
      "431 0.00014507574087474495\n",
      "432 0.00014177175762597471\n",
      "433 0.00013879949983675033\n",
      "434 0.0001358126464765519\n",
      "435 0.00013309733185451478\n",
      "436 0.0001305623445659876\n",
      "437 0.00012750361929647624\n",
      "438 0.0001252128859050572\n",
      "439 0.00012299107038415968\n",
      "440 0.00012056493142154068\n",
      "441 0.0001180994586320594\n",
      "442 0.00011568083573365584\n",
      "443 0.00011334080045344308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 0.00011123338481411338\n",
      "445 0.00010918248881353065\n",
      "446 0.0001075129330274649\n",
      "447 0.00010549947182880715\n",
      "448 0.00010350241063861176\n",
      "449 0.00010177301737712696\n",
      "450 9.977327135857195e-05\n",
      "451 9.797479287954047e-05\n",
      "452 9.625185339245945e-05\n",
      "453 9.4316725153476e-05\n",
      "454 9.250565926777199e-05\n",
      "455 9.101707837544382e-05\n",
      "456 8.938113751355559e-05\n",
      "457 8.770262502366677e-05\n",
      "458 8.63562454469502e-05\n",
      "459 8.473225170746446e-05\n",
      "460 8.343956142198294e-05\n",
      "461 8.21103822090663e-05\n",
      "462 8.071990305325016e-05\n",
      "463 7.93677318142727e-05\n",
      "464 7.798353908583522e-05\n",
      "465 7.68247336964123e-05\n",
      "466 7.57333473302424e-05\n",
      "467 7.458837353624403e-05\n",
      "468 7.31859399820678e-05\n",
      "469 7.189045572886243e-05\n",
      "470 7.056746107991785e-05\n",
      "471 6.967142689973116e-05\n",
      "472 6.852616934338585e-05\n",
      "473 6.726032006554306e-05\n",
      "474 6.65633415337652e-05\n",
      "475 6.531918188557029e-05\n",
      "476 6.440009747166187e-05\n",
      "477 6.362851854646578e-05\n",
      "478 6.269618461374193e-05\n",
      "479 6.168567051645368e-05\n",
      "480 6.0801332438131794e-05\n",
      "481 5.991800207993947e-05\n",
      "482 5.906978185521439e-05\n",
      "483 5.8338831877335906e-05\n",
      "484 5.737761239288375e-05\n",
      "485 5.658306326949969e-05\n",
      "486 5.581288496614434e-05\n",
      "487 5.4944237490417436e-05\n",
      "488 5.408786819316447e-05\n",
      "489 5.352083098841831e-05\n",
      "490 5.275545481708832e-05\n",
      "491 5.213837357587181e-05\n",
      "492 5.128693737788126e-05\n",
      "493 5.07900767843239e-05\n",
      "494 5.01224712934345e-05\n",
      "495 4.925916437059641e-05\n",
      "496 4.866668314207345e-05\n",
      "497 4.79332338727545e-05\n",
      "498 4.729688589577563e-05\n",
      "499 4.6788904001004994e-05\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10 \n",
    "#随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "learning_rate = 1e-6   \n",
    "for it in range(500):\n",
    "    # forward pass\n",
    "    h = x.mm(w1)   # N * H\n",
    "    h_relu = h.clamp(min=0)   # N * H\n",
    "    y_pred = h_relu.mm(w2)  # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(it, loss)\n",
    "    \n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    # y = ax + b\n",
    "    # dy / dx = a\n",
    "    # dy / da = x\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)    # t()  转置\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # compute weight of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor可以自动实现grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(2.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad = True)\n",
    "w = torch.tensor(2., requires_grad = True)\n",
    "b = torch.tensor(3., requires_grad = True)\n",
    "y = w*x + b    \n",
    "y.backward()\n",
    "#dy / dw = x\n",
    "print(w.grad)\n",
    "print(x.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python(pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
